
The remainder of this paper is as follows: in
Section~\ref{sec:background}, we provide an overview of quorum systems
and Dynamo-style quorums.  In Section~\ref{sec:pbs}, we introduce
Probabilistically Bounded Staleness and provide a closed-form analysis
of PBS $k$-staleness under traditional static quorums.  In
Section~\ref{sec:dynamo}, we model Dynamo-style quorum systems and
discuss when and why staleness occurs.  In
Section~\ref{sec:dynamoeval}, we use real-world workloads and Monte
Carlo analysis to determine how often Dynamo-style quorums return
stale values in practice.  In Section~\ref{sec:discussion}, we
describe further improvements to PBS partial quorums. In
Section~\ref{sec:relatedwork}, we discuss related work. In
Section~\ref{sec:conclusion}, we conclude.



To summarize our results regarding $k$-staleness, reading data that is
multiple versions stale is unlikely.  We derive a closed-form solution
for $k$-staleness for traditional quorum systems.  Our analysis
demonstrates that, as $k$ increases, there is an exponential reduction
in the probability of returning data that is staler than $k$
versions. With a modest global write rate to a particular key, a
client making read requests no greater than the write rate has a high
probability of observing monotonic reads consistency.
  
To summarize our results regarding $t$-staleness, observing staleness
across time depends on the write propagation rate and the latency
between coordinators and replicas.  In this paper, we examine
Dynamo-style partial quorums~\cite{dynamo}, currently the most widely
deployed quorum replication strategy.  We show how staleness in the
Dynamo model is a consequence of message reordering and discuss the
effect of latency variance on expected staleness across time.  We
present algorithms for asynchronous staleness detection including
simple staleness detection that is easily implementable within Dynamo
but may provide false positives and more complex, precise staleness
detection. Using production latency distributions provided by two
internet-scale companies, we analyze the likelihood of $t$-staleness
across time and replica configurations.  We find that reads observe
consistency with high probability and, tens of milliseconds after
commit, in the absence of extremely long-tailed write latencies, reads
are frequently consistent.



\begin{comment}
Distributed storage systems replicate data items to provide better
performance and availability. Many of these systems use overlapping
read and write sets, or quorums, to provide to provide consistent
reads and writes.  However, to lower the variance of user-visible
latency and handle intermittent failures, storage systems designed for
large clusters opt for ``eventual consistency,'' using \textit{non-strict}, or non-overlapping, read and write quorums. Further, operators are
required to choose a fixed number of read/write replicas to maintain
operation target latencies.
\end{comment}



\begin{comment}
and our
synthetic benchmarks show that our predicted staleness is XX\% within
the theoretical bound.  Finally, we present a case study of how a
social networking application can configure its replica management
scheme to meet its latency targets and our prototype implementation is
XX\% faster than using strict quorums.
\end{comment}


\begin{comment}
\section{Motivation}

What kind of consistency do eventually consistent data stores provide?
At its core, eventual consistency is quite weak, only guaranteeing
that the latest write will not be lost and will, at some point in the
future, reside on all replicas.  Despite the weakness of this
guarantee, many systems opt only the most basic of eventual
consistency guarantees, with no bounds on the staleness of versions
observed.  To address this deficiency, a proliferation of proposed
systems fall across on of models in the consistency spectrum, from
session guarantees~\cite{session} to causal consistency to parallel
snapshot isolation~\cite{psi}.  However, many of the most widely
deployed systems do not provide these models; simple eventual
consistency is simple to implement, if difficult to reason about.
Rather than propose another consistency model external to existing
systems, we should provide better insight into how simple eventually
consistent systems operate.  We turn to the problem of diagnosing what
consistency our existing stores provide.

\textbf{Predictability.} Can we predict the staleness of
eventually-consistent data stores? However, eventually consistent data
stores often return fairly recent versions.  We should be able to
predict the staleness of values returned from our data stores.  This
ability would be invaluable for programmers, who might like to specify
constraints on the staleness of values in terms of versions,
wall-clock time, or both.  A streaming ``news feed''-style application
can tolerate some staleness, but staleness past an hour is likely
undesirable.

\textbf{Analysis.} How consistent are eventually consistent data
stores in practice?  There is a wide range of consistency models in
the academic literature, however many commercially-deployed data
stores have instead opted for weak eventual consistency or strong
consistency guarantees instead.  Ostensibly, these systems are working
well enough for their users; ``eventually'' does not mean infinite
time in practice.  It would be helpful to characterize when these data
stores are stale and understand under which conditions these data
stores are deficient.

\textbf{Configuration.} Using a prediction mechanism, can we
automatically configure our distributed data stores for a particular
workload?  Current database configuration...


There are two main reasons to replicate data across multiple servers:
availability and scalability.  First, in the event of network
partitions, replicas on separate sides of the partition can continue
to serve (possibly stale) data.  In the event of server failure
(effectively a single-node partition), having stored the data on
multiple replicas allows end-users to continue to access the data.
The replication factor in this case depends largely on the relative
``importance'', or cost of losing the data.  Second, and of central
importance to this work, each server has a maximum capacity, or number
of requests that it can serve within a given time period.  All else
equal, replicating the data and performing appropriate load-balancing
lowers the load on each individual server storing the data.  This has
the additional benefit of lowering read and write operation latency.

However, coordinating replicas has a cost; ensuring that all replicas
are up to date is expensive.  While the distributed databases
community developed designs and algorithms for ACID-style distributed
databases for decades (CITE), in light of massive scale, internet
services instead chose to move to so-called BASE semantics (CITE).  To
achieve availability and partition tolerance, BASE systems give up
consistency.  Indeed, BASE systems cannot have all of consistency,
availability, and partition tolerance at once~\cite{cap-proof}, yet
modern BASE systems (closely affiliated with the NoSQL ``movement''
(CITE)) drop many more guarantees regarding the data they return.
Many widely-deployed open source BASE systems drop transactional
support (CITE), complex schema (CITE), and even multi-key operations
(CITE).  These storage systems are often (initially, and certainly
conceptually) simpler than traditional distributed RDBMSs and, to
their credit, scale to hundreds or thousands of machines (CITE).

In the move to BASE semantics, many data management solutions have
thrown the baby out with the bathwater, providing the bare minimum of
guarantees on data consistency.  At its core, a guarantee of eventual
consistency is a guarantee that the latest write will not be lost and
will, at some point in the future, reside on all replicas.  There is a
wide and debatable set of points in the consistency spectrum
between the most basic of eventual consistency guarantees and strong
consistency, from session guarantees to parallel snapshot isolation to
causal+ consistency and others.  While there are indeed myriad
academic designs for less-than-strongly-consistent data stores, many
industrial eventually consistent data stores themselves have done a
poor job touting their own merits.  They promise either strong
consistency or basic eventual consistency, with no guarantees
in-between.

Despite providing only the most basic of guarantees, large, web-scale,
(often) highly profitable enterprises utilize eventually consistent
systems for production services every day (CITE, CITE, CITE).
Moreover, according to our interactions with system operators, many
services deploy their data systems with eventually consistent
guarantees.  Even with a lack of sophisticated guarantees, application
writers and systems operators use these systems: either they do not
understand or care about consistency or eventual consistency is good
enough in practice.  Rather than debase the architects and adopters of
eventually consistent data storage systems, we should instead better
understand \textit{why} and when eventually consistent is good enough
for them.

If we look deeper into the operation of eventually consistent data
stores, we can begin to answer this question.  While these data stores only
explicitly provide two modes of operation, in reality, they provide
gradations of consistency out of the box, without modification.  By
modeling their (simple) internal protocols, we can provide predictions
for the degree of consistency they provide.  We can use these
predictions to inform system deployment and warn programmers about the
likelihood of corner cases (or, depending on the configuration,
common-case inconsistencies).  We can understand when deployment
conditions dictate that ``eventually consistent'' means ``all bets are
off'' and when ``eventually consistent'' means ``almost always
strongly consistent''.  We shouldn't have to guess---and we don't need
to.

\end{comment}



\begin{comment}

N: 3 R: 1 W: 2 lambda 0.000000
Avg write latency: 2.340356 (stddev: 3.320391, 99th %ile: 22.360338)
Avg read latency: 2.588911 (stddev: 6.489868, 99th %ile: 30.617551)

N: 3 R: 1 W: 3 lambda 0.000000
Avg write latency: 4.412850 (stddev: 8.626731, 99th %ile: 50.466737)
Avg read latency: 2.277779 (stddev: 6.003202, 99th %ile: 28.037702)

N: 3 R: 2 W: 1 lambda 0.000000
Avg write latency: 1.786696 (stddev: 2.229738, 99th %ile: 6.011726)
Avg read latency: 2.047036 (stddev: 5.385266, 99th %ile: 23.046278)

N: 3 R: 2 W: 2 lambda 0.000000
Avg write latency: 2.314494 (stddev: 3.058672, 99th %ile: 16.398516)
Avg read latency: 2.068930 (stddev: 5.495067, 99th %ile: 22.539867)

N: 3 R: 3 W: 1 lambda 0.000000
Avg write latency: 1.869139 (stddev: 3.258769, 99th %ile: 14.491754)
Avg read latency: 3.309529 (stddev: 8.348639, 99th %ile: 44.156379)

\subsection{Operation Latency}
\label{sec:real-latency}

In our measurements on EC2, the correllation of writes was MADE UP,
meaning the operations were effectively IID.  However, we can
approximate an environment with a higher correlation by modifying $w$.
In an environment where half of the nodes are co-located on a rack, we can treat $w$ as $w/2$.  We show the tradeoffs between correlation and latency in Figure \ref{fig:correlation}.

\begin{figure}
\centering
\includegraphics[width=.8\columnwidth]{figs/correlation.pdf}
\caption{Expected latency of write quorums of $w$ replicas in Dynamo-style operation versus quorum element correlation.}
\label{fig:correlation}
\end{figure}

\subsection{Optimization and SLAs}
\label{sec:optimization}

John Wilkes--Omega and ``intentions''

\begin{figure}
\centering
\includegraphics[width=.8\columnwidth]{figs/bothdimensions.pdf}
\caption{Time and version staleness required to ensure varying SLA constraints on the probability of returning staler-than-promised data in simple microbenchmarking.}
\label{fig:prob-staler}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=.8\columnwidth]{figs/inaccuracy.pdf}
\caption{Accuracy of model compared to experimental microbenchmarking.
  The size of each point is linearly proportional to the inaccuracy of
  the model's predictions.}
\label{fig:prob-staler}
\end{figure}



\subsection{Twitter Clone}

\begin{figure}
\centering
\includegraphics[width=.8\columnwidth]{figs/twissandra.pdf}
\caption{Time and version staleness tradeoffs as measured by end users
  of Twitter clone.}
\label{fig:twissandra}
\end{figure}

\end{comment}


\subsection{Optimization Formulation}

With these equations, we can optimize the selection of a partial
quorum system to minimize overall operation latency subject to
constraints on staleness. We can guarantee bounded staleness by
ensuring that $p_{staler} = 1$, however this is only possible in
$k$-quorum consistency as the real-time components of $RT$-quorum
consistency and its variants are inherently probabilistic (unless we
can prove an absolute bound on operation latency).  We can use a
simply formulated optimization program to determine what combination
of $r$ and $w$ minimizes latency while satisfying the system
constraints and user requirements.

Given $n$, desired $p$, $w_{min}$ (minimum durability of writes),
consistency model (with necessary parameters--$k$, $t$, $\gamma_{cr}$,
etc.), and the relative weighted ``importances'' of read
and write latency, $c_r$ and $c_w$, we can determine $r$ and $w$:

\begin{equation}
 \begin{array}{rl}
    \min        & c_r\cdot RQ_l(r) +c_w \cdot WQ_l(w) \\
    \mbox{s.t.} & p \ge p_{staler} \\
                & w \ge w_{min}.
    \end{array}
\end{equation}

We implement and validate this optimization framework in
Section~\ref{sec:optimization}.

\subsection{Typical Quorum Configurations}

CASSANDA: R=1, W=1 

<<<<<<< HEAD
RIAK: N=3, R,W QUORUM 
=======
RIAK: N=3, R,W QUORUM \url{https://github.com/basho/riak_kv/blob/1.0/src/riak_kv_app.erl}
\url{http://wiki.basho.com/Riak-Glossary.html#Quorum}
>>>>>>> 11fb27c39c7f11f1b544d903fd16543cac89854b

Voldemort: does not provide production configs besides testing (N=2)



Until now, we have assumed that the expected latency of operations
reaching a given number of replicas by a particular time
($p_w(\mathcal{W}, t)$ for writes) in the case of writes is known.  We
can measure this distribution empirically (Section
\ref{sec:real-latency}), but we can also model this latency
analytically, given a few assumptions.

To begin, we consider write latency.  With Dynamo-style queries, we
want to determine the probability that $w$ of the replicas in the
write quorum $\mathcal{Q}$ respond within time $t$.  We can determine
this in at least two ways.  One way is to find use order statistic
theory to determine the probability that the $w$th replica reasponds
by time $t$.  However, this is equivalent to taking the minimum
response time over the maximum of each possible set of $w$ replicas
from $\mathcal{Q}$, which works out more cleanly in this formulation.
Given the write latency cumulative density functions for a set of
nodes $S$, $L_{ws}(S, t)$, the probability density function $p_w$ of
write latency of a set of nodes is:
\begin{equation}
p_w(\mathcal{W}, t) = min(\{L_{ws}(S, t) \mid S \subseteq \mathcal{Q}, |S| = \mathcal{W}\})
\end{equation}


$\mathcal{W}$, enoted $WQ_l(\mathcal{W})$, we simply need to consider
the conditional probability of $\mathcal{W}$ writes completing across
all possible time values $t$.  However, this depends on the
distribution of $L_{ws}(S, t)$, which requires some
assumptions. Determining the ordinal probability of a set of randomly
distributed variables (in our case, the minimum of all $L_{ws}(S,
t)$), is possible but is also fairly complicated.  The difficult
arises in determining the distribution of a given $L_{ws}(S)$.  If we
assume that all nodes in $S$ obey independent latency distributions,
this is tractible.  If we assume that they are dependent and therefore
obey a joint distribution, then, as $w$ grows, solving for this
probability becomes much more difficult~\cite{needed}.  Approximation
algorithms can assist here~\cite{needed}, but we can also make
simplifying assumptions about the distribution of these latencies.

If we assume that latencies are independently, identically distributed
(IID), that is, if each node obeys the same latency distribution and
each nodes latency is independent of the other nodes's latency, this
equation is greatly simplified.  Under IID assumptions, the time for a
write to reach every node in the quorum obeys a single latency
CDF\footnote{Note that this distribution only captures the request
  forward, not the round-trip time before the acknowledgement.  The
  replica can serve the data before it responds to the proxy.},
$L_w(t)$ (PDF, resp. $l_w(t)$).  The probability that $w$ nodes all
respond within time $t$ is simply $(L_w(t))^\mathcal{W}$, and the
minimum over all such of these sets is the CDF $P_w$:
\begin{equation}
P_w(\mathcal{W}, t) = 1-(1-(L_w(t))^\mathcal{W})^{n \choose \mathcal{W}}
\end{equation}
This is indeed simplifying, but it makes the theory much simpler.  We
discuss the validity of this IID assumption when we measure latencies
experimentally in Section \ref{sec:real-latency}.

Under the IID assumption, we can easily determine the expected
Dynamo-style write quorum operation latency:
\begin{equation}
WQ_l(\mathcal{W}) = \int_0^{\infty} t \cdot p_w(\mathcal{W}, t) dt
\end{equation}
In practice, the proxy is often a replica as well, so for a write
quorum of size $w$, we only need to consider $\mathcal{W}=w-1$ writes.

Similarly, to determine how many replicas have a particular value (even if they have not acked) after a write quorum of size $w$, we evaluate:
\begin{equation}
P_{w:have}(\mathcal{W}, t) = 1-(1-(L_w(t))^{\mathcal{W}-w})^{n-w \choose \mathcal{W}-w}
\end{equation}

Thus far, we have only considered the write operation latencies.  The
read quorum latency may be different from the write latency depending
on node-level actions such as forced logging or disk-bound operations.
However, the expected read operation latency for a quorum of size
$\mathcal{R}$, $RQ_l(\mathcal{R})$, can be calculated similarly given a
model for the latency of reads ($L_{rs}(S,t)$ and $L_r(t))$.



\begin{comment}
\begin{appendix}
\section{Dynamo-style Staleness}

We present an analytical closed-form solution to $p_{staler}$ for
$t$-visible partial quorums unde Dynamo-style.  What is the
probability that a read result is not strongly consistent (that is,
does not return the most recent value when the read began)? In this
section, when we refer to the ``last committed version'', we are
referring to the last committed version before the read began. We
consider the pathological case where a read occurs immediately after a
write occurs.

We assume that each writes to a replica is independently delayed by a
randomly distributed variate $W$, each write acknowledgement from a
replica to a writer is independently delayed by a randomly distributed
variate $A$, each read request to a replica is independently delayed
by a randomly distributed variate $R$, and each read response from a
replica to a reader is delayed according to a randomly distributed
variate $S$.  We make no assumptions as to the distribution of $W,$
$A,$ $R,$ and $S$, except that their domain is strictly positive (no
negative delay) and that values drawn from each are independently,
identically distributed (IID).

The probability that a read quorum does not contain the last committed
version is equivalent to the probability that each of the first $R$
read responses are all stale.  Denote the probability that the $i$th
response is stale as $stale_i$:
\begin{equation*}
p_{staler} = \prod_{i=0}^{R} \Pr(stale_i)
\end{equation*}

Denote the return time of the $i$th read response as $S_i$.  The probability that the $i$th response is stale is given by:
\begin{equation*}
\Pr(stale_i) = \int_{0}^{\infty} \Pr(S_i = t) \cdot \Pr(stale_i \mid S_i = t) dt
\end{equation*}

We can use simple order statistics to calculate the return time of the $i$th read response:
\begin{multline*}
\Pr(S_i = t) = \frac{N!}{(i-1)!(N-i)!}\cdot[\Pr(R+S< t)]^{i-1}\\\cdot\Pr(R+S = t)\cdot[\Pr(R+S > t)]^{N-i}
\end{multline*}

 Recall that under Dynamo-style messaging, read response from a
 replica is stale if the read request arrived at the replica before
 the write command for the last committed version reached the replica.
 Denote the time required to commit the last version (alternatively, the time required for $W$ of $N$ replicas to acknowledge the last write) as $w_t$.
\begin{multline*}
\Pr(stale_i \mid S_i = t) = \int_{0}^{t} \Pr(R=\tau)\cdot\Pr(S=t-\tau)\\\cdot\Pr(R+w_t < W \mid R = \tau) d\tau
\end{multline*}
\begin{multline*}
  \Pr(R+w_t < W \mid R = \tau) = \int_{\tau}^{\infty} \Pr(W-w_t = x) dx
\end{multline*}
\begin{multline*}
  \Pr(W-w_t = z) = \int_{0}^{\infty} \Pr(W = y) \cdot \Pr(-w_t = z-y) dy
\end{multline*}
Finally, to calculate $w_t$, we calculate the probabilty that
$\mathcal{W}$ of $N$ acknowledgements come in at time $t$:
\begin{multline*}
\Pr(w_t = t) = \frac{N!}{(\mathcal{W}-1)!(N-\mathcal{W})!}\cdot[\Pr(W+A< t)]^{\mathcal{W}-1}\\\cdot\Pr(W+A = t)\cdot[\Pr(W+A > t)]^{N-\mathcal{W}}
\end{multline*}
Combining these equations, we have a closed-form solution for the
staleness dependent on $W,$ $A,$ $R,$ and $S$, as desired.  To consider
multiple version staleness, exponentiate $p_s$ by the number of
versions stale.

\end{appendix}
\end{comment}



In the language of the probabilistic quorum literature, we have
constructed a PBS $k$-quorum from $k$ $\varepsilon$-quorum systems
where $\varepsilon \leq \sqrt[k]{p_{staler}}$. This system has
load~\cite[Definition 3.2]{quorumsystems} $\geq
\frac{1-\varepsilon^{\frac{1}{2k}}}{\sqrt{n}}$, an exponentially lower
bound than a strict probabilistic quorum.  This follows immediately
from~\cite[Corollary 3.12]{prob-quorum}.


When
constructed from $k$ $\varepsilon$-consistent systems (as we have
here), this consistency model has load $\geq
\frac{(1-p_{staler}^{\frac{1}{2C}})}{\sqrt{N}}$, where
$C=\frac{\gamma_{gw}}{\gamma_{cr}}$.


This prior work has two properties with important implications for
practitioners.  First, existing theory treats quorum sizes as fixed; a
write quorum is chosen and no other replicas in the system
subsequently learn about the value unless it is written again.  The
theory does not account for propagation of versions across time and
between replicas (anti-entropic processes).  Second, much of this
prior work assumes Byzantine failure.  If the description of prior
work seemed simplistic, it is largely because most of the literature
content addresses problems such as adversarial quorum selection and
scheduling.  In this work, we re-evaluate both of these assumptions.
We elaborate further in the next subsection.


One major challenge in
performing our experiments was clock skew across replicas; we relied
on both \texttt{ntp} and user-level skew measurements to bound these
drifts.  This is a recently studied problem, particularly under VM
migration and in the Xen hypervisor~\cite{time-virt}. In other
experiments, simulation avoided these complexities.

Across multiple latency profiles, we were able to predict staleness
with extremely high accuracy. Under default settings, we did not
observe any staleness due to the limited operation latency; message
reordering was not a problem.  Using the operation delays recorded in
Cassandra across 5000 writes, we reconstructed the latency
distributions and repeated each experiment in simulation. 